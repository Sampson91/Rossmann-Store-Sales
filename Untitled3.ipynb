{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import csv\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import itertools\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import cross_validation\n",
    "from matplotlib import pylab as plt\n",
    "plot = True\n",
    "goal = 'Sales'\n",
    "myid = 'Id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ToWeight(y):\n",
    "    w = np.zeros(y.shape,dtype=float)\n",
    "    ind = y!=0\n",
    "    w[ind] = 1./(y[ind]**2)\n",
    "    return w\n",
    "    \n",
    "def rmspe(yhat,y):\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean(w*(y-yhat)**2))\n",
    "    return rmspe\n",
    "\n",
    "def rmspe_xg(yhat,y):\n",
    "    y = y.get_label() \n",
    "    y = np.exp(y) -1\n",
    "    yhat = np.exp(yhat)-1\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean(w*(y - yhat)**2))\n",
    "    return 'rmspe',rmspe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1115 entries, 0 to 1114\n",
      "Data columns (total 10 columns):\n",
      "Store                        1115 non-null int64\n",
      "StoreType                    1115 non-null object\n",
      "Assortment                   1115 non-null object\n",
      "CompetitionDistance          1112 non-null float64\n",
      "CompetitionOpenSinceMonth    761 non-null float64\n",
      "CompetitionOpenSinceYear     761 non-null float64\n",
      "Promo2                       1115 non-null int64\n",
      "Promo2SinceWeek              571 non-null float64\n",
      "Promo2SinceYear              571 non-null float64\n",
      "PromoInterval                571 non-null object\n",
      "dtypes: float64(5), int64(2), object(3)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "store = pd.read_csv('./store.csv')\n",
    "store.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41088 entries, 0 to 41087\n",
      "Data columns (total 8 columns):\n",
      "Id               41088 non-null int64\n",
      "Store            41088 non-null int64\n",
      "DayOfWeek        41088 non-null int64\n",
      "Date             41088 non-null object\n",
      "Open             41077 non-null float64\n",
      "Promo            41088 non-null int64\n",
      "StateHoliday     41088 non-null object\n",
      "SchoolHoliday    41088 non-null int64\n",
      "dtypes: float64(1), int64(5), object(2)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('./test.csv')\n",
    "\n",
    "test_df.head()\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    store = pd.read_csv('./store.csv')\n",
    "    train_org = pd.read_csv('./train.csv',dtype={'StateHoliday':pd.np.string_})\n",
    "    test_org = pd.read_csv('./test.csv',dtype={'StateHoliday':pd.np.string_})\n",
    "    train = pd.merge(train_org,store,on='Store',how='left')\n",
    "    test = pd.merge(test_org,store,on='Store',how='left')\n",
    "    feature = test.columns.tolist()\n",
    "    numerics = ['int16','int32','int64','float16','float32','float64']\n",
    "    feature_numeric = test.select_dtypes(include = numerics).columns.tolist()\n",
    "    feature_non_numeric = [f for f in feature if f not in feature_numeric]\n",
    "    return (train,test,feature,feature_non_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_data(train,test,features,features_non_numeric):\n",
    "    train = train[train['Sales']>0]\n",
    "\n",
    "    for data in [train,test]:\n",
    "        data['year'] = data.Date.apply(lambda x : x.split('-')[0])\n",
    "        data['year'] = data['year'].astype(float)\n",
    "        data['month']= data.Date.apply(lambda x : x.split('-')[1])\n",
    "        data['month']= data['month'].astype(float)\n",
    "        data['day']  = data.Date.apply(lambda x : x.split('-')[2])\n",
    "        data['day']  = data['day'].astype(float)\n",
    "        \n",
    "        data['promojan'] = data.PromoInterval.apply(lambda x:0 if isinstance(x,float) else 1 if 'Jan' in x else 0)\n",
    "        data['promofeb'] = data.PromoInterval.apply(lambda x:0 if isinstance(x,float) else 1 if 'Feb' in x else 0)\n",
    "        data['promomar'] = data.PromoInterval.apply(lambda x:0 if isinstance(x,float) else 1 if 'Mar' in x else 0)\n",
    "        data['promoapr'] = data.PromoInterval.apply(lambda x:0 if isinstance(x,float) else 1 if 'Apr' in x else 0)\n",
    "        data['promomay'] = data.PromoInterval.apply(lambda x:0 if isinstance(x,float) else 1 if 'May' in x else 0)\n",
    "        data['promojun'] = data.PromoInterval.apply(lambda x:0 if isinstance(x,float) else 1 if 'Jun' in x else 0)\n",
    "        data['promojul'] = data.PromoInterval.apply(lambda x:0 if isinstance(x,float) else 1 if 'Jul' in x else 0)\n",
    "        data['promoaug'] = data.PromoInterval.apply(lambda x:0 if isinstance(x,float) else 1 if 'Aug' in x else 0)\n",
    "        data['promosep'] = data.PromoInterval.apply(lambda x:0 if isinstance(x,float) else 1 if 'Sep' in x else 0)\n",
    "        data['promooct'] = data.PromoInterval.apply(lambda x:0 if isinstance(x,float) else 1 if 'Oct' in x else 0)\n",
    "        data['promonov'] = data.PromoInterval.apply(lambda x:0 if isinstance(x,float) else 1 if 'Nov' in x else 0)\n",
    "        data['promodec'] = data.PromoInterval.apply(lambda x:0 if isinstance(x,float) else 1 if 'Dec' in x else 0)\n",
    "        \n",
    "    noisy_features = [myid,'Date']\n",
    "    features = [c for c in features if c not in noisy_features]\n",
    "    features_non_numeric = [c for c in features_non_numeric if c not in noisy_features]\n",
    "    features.extend(['year','month','day'])\n",
    "    class DataFrameInputer(TransformerMixin):\n",
    "        \n",
    "        def __init__(self):\n",
    "                \"\"\"\n",
    "                \"\"\"\n",
    "        def fit(self, X, y=None):\n",
    "            self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "                                      if X[c].dtype==np.dtype('O') else X[c].mean() for c in X],\n",
    "                                      index = X.columns)\n",
    "            return self\n",
    "        def transform(self,X,y=None):\n",
    "            return X.fillna(self.fill)\n",
    "        \n",
    "    train = DataFrameInputer().fit_transform(train)\n",
    "    test  = DataFrameInputer().fit_transform(test)\n",
    "        \n",
    "    le = LabelEncoder()\n",
    "    for col in features_non_numeric:\n",
    "        le.fit(list(train[col])+list(test[col]))\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col] = le.transform(test[col])\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    for col in set(features) - set(features_non_numeric) - set([]):\n",
    "        try:\n",
    "            scaler.fit(list(train[col])+list(test[col]))\n",
    "        except:\n",
    "            print(col)\n",
    "        train[col] = scaler.transform(train[col])\n",
    "        test[col] = scaler.transform(test[col])\n",
    "    return (train,test,features,features_non_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1017209 entries, 0 to 1017208\n",
      "Data columns (total 18 columns):\n",
      "Store                        1017209 non-null int64\n",
      "DayOfWeek                    1017209 non-null int64\n",
      "Date                         1017209 non-null object\n",
      "Sales                        1017209 non-null int64\n",
      "Customers                    1017209 non-null int64\n",
      "Open                         1017209 non-null int64\n",
      "Promo                        1017209 non-null int64\n",
      "StateHoliday                 1017209 non-null object\n",
      "SchoolHoliday                1017209 non-null int64\n",
      "StoreType                    1017209 non-null object\n",
      "Assortment                   1017209 non-null object\n",
      "CompetitionDistance          1014567 non-null float64\n",
      "CompetitionOpenSinceMonth    693861 non-null float64\n",
      "CompetitionOpenSinceYear     693861 non-null float64\n",
      "Promo2                       1017209 non-null int64\n",
      "Promo2SinceWeek              509178 non-null float64\n",
      "Promo2SinceYear              509178 non-null float64\n",
      "PromoInterval                509178 non-null object\n",
      "dtypes: float64(5), int64(8), object(5)\n",
      "memory usage: 147.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train,test,features,features_non_numeric = load_data()\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          2008.0\n",
       "1          2007.0\n",
       "2          2006.0\n",
       "3          2009.0\n",
       "4          2015.0\n",
       "5          2013.0\n",
       "6          2013.0\n",
       "7          2014.0\n",
       "8          2000.0\n",
       "9          2009.0\n",
       "10         2011.0\n",
       "11            NaN\n",
       "12            NaN\n",
       "13         2014.0\n",
       "14         2010.0\n",
       "15            NaN\n",
       "16         2005.0\n",
       "17         2010.0\n",
       "18            NaN\n",
       "19         2009.0\n",
       "20         1999.0\n",
       "21            NaN\n",
       "22         2005.0\n",
       "23         2000.0\n",
       "24         2003.0\n",
       "25            NaN\n",
       "26         2005.0\n",
       "27         2014.0\n",
       "28            NaN\n",
       "29         2014.0\n",
       "            ...  \n",
       "1017179    2013.0\n",
       "1017180    2011.0\n",
       "1017181    2009.0\n",
       "1017182    2009.0\n",
       "1017183       NaN\n",
       "1017184       NaN\n",
       "1017185    2000.0\n",
       "1017186    2009.0\n",
       "1017187    2013.0\n",
       "1017188    2007.0\n",
       "1017189       NaN\n",
       "1017190    2002.0\n",
       "1017191    2004.0\n",
       "1017192    2013.0\n",
       "1017193       NaN\n",
       "1017194    2012.0\n",
       "1017195    2012.0\n",
       "1017196    2006.0\n",
       "1017197    2012.0\n",
       "1017198    2008.0\n",
       "1017199    2011.0\n",
       "1017200    2012.0\n",
       "1017201    2004.0\n",
       "1017202    2011.0\n",
       "1017203    2010.0\n",
       "1017204    2014.0\n",
       "1017205    2006.0\n",
       "1017206       NaN\n",
       "1017207       NaN\n",
       "1017208       NaN\n",
       "Name: CompetitionOpenSinceYear, dtype: float64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.fillna(0)\n",
    "train.groupby('CompetitionOpenSinceMonth').count()\n",
    "train['CompetitionOpenSinceYear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         -0.140119\n",
       "1         -0.341682\n",
       "2         -0.543245\n",
       "3          0.061444\n",
       "4          1.270821\n",
       "5          0.867695\n",
       "6          0.867695\n",
       "7          1.069258\n",
       "8         -1.752622\n",
       "9          0.061444\n",
       "10         0.464570\n",
       "11         0.000522\n",
       "12         0.000522\n",
       "13         1.069258\n",
       "14         0.263007\n",
       "15         0.000522\n",
       "16        -0.744808\n",
       "17         0.263007\n",
       "18         0.000522\n",
       "19         0.061444\n",
       "20        -1.954185\n",
       "21         0.000522\n",
       "22        -0.744808\n",
       "23        -1.752622\n",
       "24        -1.147933\n",
       "25         0.000522\n",
       "26        -0.744808\n",
       "27         1.069258\n",
       "28         0.000522\n",
       "29         1.069258\n",
       "             ...   \n",
       "1016082   -0.543245\n",
       "1016083    0.666133\n",
       "1016084   -0.140119\n",
       "1016085    0.464570\n",
       "1016086    0.666133\n",
       "1016087   -0.946371\n",
       "1016088    0.464570\n",
       "1016089    0.263007\n",
       "1016090    1.069258\n",
       "1016091   -0.543245\n",
       "1016092    0.000522\n",
       "1016093    0.000522\n",
       "1016094    0.000522\n",
       "1016179    0.464570\n",
       "1016353    0.000522\n",
       "1016356    0.867695\n",
       "1016368    0.000522\n",
       "1016429    0.000522\n",
       "1016447    0.000522\n",
       "1016517    1.069258\n",
       "1016588    0.464570\n",
       "1016606    0.000522\n",
       "1016624    0.000522\n",
       "1016656    0.000522\n",
       "1016770   -0.140119\n",
       "1016776   -0.543245\n",
       "1016827   -1.954185\n",
       "1016863    0.000522\n",
       "1017042    0.000522\n",
       "1017190   -1.349496\n",
       "Name: CompetitionOpenSinceYear, dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,test,features,features_non_numeric = process_data(train,test,features,features_non_numeric)\n",
    "train['CompetitionOpenSinceYear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def XGB_native(train,test,features,features_non_numeric):\n",
    "    depth =13\n",
    "    eta = 0.01\n",
    "    ntrees = 3000\n",
    "    mcw = 3\n",
    "    params = {'objective':'reg:linear',\n",
    "             'booster':'gbtree',\n",
    "             'eta':eta,\n",
    "              'mx_depth':depth,\n",
    "              'min_child_weight':mcw,\n",
    "              'subsample':0.9,\n",
    "              'colsample_bytree':0.7,\n",
    "              'silent':1\n",
    "             }\n",
    "    print \"params\"+str(params)\n",
    "    print 'featres'+str(features)\n",
    "    \n",
    "    tsize = 0.05\n",
    "    X_train, X_test = cross_validation.train_test_split(train, test_size=tsize)\n",
    "    dtrain = xgb.DMatrix(X_train[features], np.log(X_train[goal] + 1))\n",
    "    dvalid = xgb.DMatrix(X_test[features], np.log(X_test[goal] + 1))\n",
    "    watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "#     gbm = xgb.train(params,dtrain,ntrees,evals=watchlist,early_stopping_rounds=100,feval=rmspe_xg,verbose_eval=True)\n",
    "    gbm = xgb.train(params, dtrain, ntrees, evals=watchlist, early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)\n",
    "   \n",
    "    indices = train_probs<0\n",
    "    train_probs[indices] = 0\n",
    "    error = rmspe(np.exp(train_probs)-1,X_test[goal].values)\n",
    "    print \"````````\",error\n",
    "    \n",
    "    test_probs = gbm.predict(xgb.DMatrix(test[features]))\n",
    "    indices = test_probs<0\n",
    "    test_probs[indices] = 0\n",
    "    submission = pd.DataFrame({myid:test[myid],goal:np.exp(test_probs)-1})\n",
    "    if os.path.exists('result/'):\n",
    "        os.makedirs('result/')\n",
    "    submission.to_csv('./result/data_xgb_d%s_eta%s_ntree%s_mcw%s_tsze%s.csv'%(str(depth),str(eta),str(ntrees),str(mcw),str(tsize)),index=False)\n",
    "    \n",
    "    if plot:\n",
    "        outfile = open('xgb.fmap','w')\n",
    "        i = 0\n",
    "        for feat in features:\n",
    "            outfile.write('{0}\\t{1}\\tq\\n'.format(i,feat))\n",
    "            i = i +1\n",
    "        outfile.close()\n",
    "        importance = gbm.get_fscore(fmap='xgb.fmap')\n",
    "        importance = sorted(importance.items(),key = operator.itemgetter(1))\n",
    "        df = pd.DataFrame(importance,columns=['feature','fscore'])\n",
    "        df['fscore'] = df['fscore'] /df['fscore'].sum()\n",
    "        \n",
    "        plt.figure()\n",
    "        df.plot()\n",
    "        df.plot(kind = 'barh',x = 'feature',y = 'fscore',legend=False,figsize = (25,15))\n",
    "        plt.title('XGBoost Feature Importance')\n",
    "        plt.xlabel('relative importance')\n",
    "        plt.gcf.savefig('feature_importance_xgb_d%s_eta%s_mvw%s_tsize%s.png'%(str(depth),str(eta),str(ntrees),str(mcw),str(tsize)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train,test,features,features_non_numeric = load_data()\n",
    "# train,test,features,features_non_numeric = process_data(train,test,features,features_non_numeric)\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params{'subsample': 0.9, 'eta': 0.01, 'colsample_bytree': 0.7, 'silent': 1, 'objective': 'reg:linear', 'mx_depth': 13, 'min_child_weight': 3, 'booster': 'gbtree'}\n",
      "featres['Store', 'DayOfWeek', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval', 'year', 'month', 'day']\n",
      "[0]\teval-rmspe:0.999864\ttrain-rmspe:0.999864\n",
      "Multiple eval metrics have been passed: 'train-rmspe' will be used for early stopping.\n",
      "\n",
      "Will train until train-rmspe hasn't improved in 100 rounds.\n",
      "[1]\teval-rmspe:0.999837\ttrain-rmspe:0.999837\n",
      "[2]\teval-rmspe:0.999809\ttrain-rmspe:0.999809\n",
      "[3]\teval-rmspe:0.999779\ttrain-rmspe:0.999779\n",
      "[4]\teval-rmspe:0.999747\ttrain-rmspe:0.999747\n",
      "[5]\teval-rmspe:0.999712\ttrain-rmspe:0.999712\n",
      "[6]\teval-rmspe:0.999675\ttrain-rmspe:0.999675\n",
      "[7]\teval-rmspe:0.999636\ttrain-rmspe:0.999636\n",
      "[8]\teval-rmspe:0.999593\ttrain-rmspe:0.999594\n",
      "[9]\teval-rmspe:0.999548\ttrain-rmspe:0.999548\n",
      "[10]\teval-rmspe:0.9995\ttrain-rmspe:0.9995\n",
      "[11]\teval-rmspe:0.999449\ttrain-rmspe:0.999449\n",
      "[12]\teval-rmspe:0.999394\ttrain-rmspe:0.999394\n",
      "[13]\teval-rmspe:0.999336\ttrain-rmspe:0.999336\n",
      "[14]\teval-rmspe:0.999274\ttrain-rmspe:0.999274\n",
      "[15]\teval-rmspe:0.999208\ttrain-rmspe:0.999209\n",
      "[16]\teval-rmspe:0.999139\ttrain-rmspe:0.999139\n",
      "[17]\teval-rmspe:0.999064\ttrain-rmspe:0.999065\n",
      "[18]\teval-rmspe:0.998986\ttrain-rmspe:0.998986\n",
      "[19]\teval-rmspe:0.998902\ttrain-rmspe:0.998903\n",
      "[20]\teval-rmspe:0.998814\ttrain-rmspe:0.998815\n",
      "[21]\teval-rmspe:0.998721\ttrain-rmspe:0.998721\n",
      "[22]\teval-rmspe:0.998622\ttrain-rmspe:0.998623\n",
      "[23]\teval-rmspe:0.998518\ttrain-rmspe:0.998518\n",
      "[24]\teval-rmspe:0.998407\ttrain-rmspe:0.998408\n",
      "[25]\teval-rmspe:0.998291\ttrain-rmspe:0.998292\n",
      "[26]\teval-rmspe:0.998168\ttrain-rmspe:0.998168\n",
      "[27]\teval-rmspe:0.998038\ttrain-rmspe:0.998039\n",
      "[28]\teval-rmspe:0.997902\ttrain-rmspe:0.997903\n",
      "[29]\teval-rmspe:0.997758\ttrain-rmspe:0.997759\n",
      "[30]\teval-rmspe:0.997606\ttrain-rmspe:0.997608\n",
      "[31]\teval-rmspe:0.997447\ttrain-rmspe:0.997448\n",
      "[32]\teval-rmspe:0.99728\ttrain-rmspe:0.997281\n",
      "[33]\teval-rmspe:0.997104\ttrain-rmspe:0.997106\n",
      "[34]\teval-rmspe:0.99692\ttrain-rmspe:0.996921\n",
      "[35]\teval-rmspe:0.996727\ttrain-rmspe:0.996728\n",
      "[36]\teval-rmspe:0.996523\ttrain-rmspe:0.996525\n",
      "[37]\teval-rmspe:0.996311\ttrain-rmspe:0.996313\n",
      "[38]\teval-rmspe:0.996088\ttrain-rmspe:0.996091\n",
      "[39]\teval-rmspe:0.995856\ttrain-rmspe:0.995858\n",
      "[40]\teval-rmspe:0.995612\ttrain-rmspe:0.995614\n",
      "[41]\teval-rmspe:0.995357\ttrain-rmspe:0.995359\n",
      "[42]\teval-rmspe:0.99509\ttrain-rmspe:0.995093\n",
      "[43]\teval-rmspe:0.994812\ttrain-rmspe:0.994815\n",
      "[44]\teval-rmspe:0.994522\ttrain-rmspe:0.994525\n",
      "[45]\teval-rmspe:0.994219\ttrain-rmspe:0.994222\n",
      "[46]\teval-rmspe:0.993903\ttrain-rmspe:0.993906\n",
      "[47]\teval-rmspe:0.993574\ttrain-rmspe:0.993577\n",
      "[48]\teval-rmspe:0.993231\ttrain-rmspe:0.993235\n",
      "[49]\teval-rmspe:0.992874\ttrain-rmspe:0.992878\n",
      "[50]\teval-rmspe:0.992503\ttrain-rmspe:0.992507\n",
      "[51]\teval-rmspe:0.992117\ttrain-rmspe:0.992121\n",
      "[52]\teval-rmspe:0.991715\ttrain-rmspe:0.991719\n",
      "[53]\teval-rmspe:0.991298\ttrain-rmspe:0.991302\n",
      "[54]\teval-rmspe:0.990864\ttrain-rmspe:0.990869\n",
      "[55]\teval-rmspe:0.990414\ttrain-rmspe:0.990419\n",
      "[56]\teval-rmspe:0.989946\ttrain-rmspe:0.989951\n",
      "[57]\teval-rmspe:0.989462\ttrain-rmspe:0.989467\n",
      "[58]\teval-rmspe:0.98896\ttrain-rmspe:0.988966\n",
      "[59]\teval-rmspe:0.98844\ttrain-rmspe:0.988446\n",
      "[60]\teval-rmspe:0.987901\ttrain-rmspe:0.987907\n",
      "[61]\teval-rmspe:0.987343\ttrain-rmspe:0.987349\n",
      "[62]\teval-rmspe:0.986764\ttrain-rmspe:0.986772\n",
      "[63]\teval-rmspe:0.986166\ttrain-rmspe:0.986174\n",
      "[64]\teval-rmspe:0.985549\ttrain-rmspe:0.985558\n",
      "[65]\teval-rmspe:0.98491\ttrain-rmspe:0.984919\n",
      "[66]\teval-rmspe:0.984252\ttrain-rmspe:0.984261\n",
      "[67]\teval-rmspe:0.983572\ttrain-rmspe:0.983582\n",
      "[68]\teval-rmspe:0.982869\ttrain-rmspe:0.98288\n",
      "[69]\teval-rmspe:0.982146\ttrain-rmspe:0.982156\n",
      "[70]\teval-rmspe:0.981399\ttrain-rmspe:0.98141\n",
      "[71]\teval-rmspe:0.980629\ttrain-rmspe:0.980641\n",
      "[72]\teval-rmspe:0.979835\ttrain-rmspe:0.979848\n",
      "[73]\teval-rmspe:0.979017\ttrain-rmspe:0.979031\n",
      "[74]\teval-rmspe:0.978177\ttrain-rmspe:0.978191\n",
      "[75]\teval-rmspe:0.977309\ttrain-rmspe:0.977323\n",
      "[76]\teval-rmspe:0.976419\ttrain-rmspe:0.976434\n",
      "[77]\teval-rmspe:0.975504\ttrain-rmspe:0.97552\n",
      "[78]\teval-rmspe:0.974564\ttrain-rmspe:0.97458\n",
      "[79]\teval-rmspe:0.973595\ttrain-rmspe:0.973613\n",
      "[80]\teval-rmspe:0.972603\ttrain-rmspe:0.972622\n",
      "[81]\teval-rmspe:0.971584\ttrain-rmspe:0.971603\n",
      "[82]\teval-rmspe:0.970538\ttrain-rmspe:0.970558\n",
      "[83]\teval-rmspe:0.969462\ttrain-rmspe:0.969483\n",
      "[84]\teval-rmspe:0.968362\ttrain-rmspe:0.968383\n",
      "[85]\teval-rmspe:0.967233\ttrain-rmspe:0.967255\n",
      "[86]\teval-rmspe:0.966074\ttrain-rmspe:0.966098\n",
      "[87]\teval-rmspe:0.964889\ttrain-rmspe:0.964914\n",
      "[88]\teval-rmspe:0.963674\ttrain-rmspe:0.963701\n",
      "[89]\teval-rmspe:0.962432\ttrain-rmspe:0.96246\n",
      "[90]\teval-rmspe:0.961158\ttrain-rmspe:0.961188\n",
      "[91]\teval-rmspe:0.959856\ttrain-rmspe:0.959887\n",
      "[92]\teval-rmspe:0.958525\ttrain-rmspe:0.958558\n",
      "[93]\teval-rmspe:0.957164\ttrain-rmspe:0.957198\n",
      "[94]\teval-rmspe:0.955774\ttrain-rmspe:0.955809\n",
      "[95]\teval-rmspe:0.954352\ttrain-rmspe:0.954389\n",
      "[96]\teval-rmspe:0.952897\ttrain-rmspe:0.952938\n",
      "[97]\teval-rmspe:0.951416\ttrain-rmspe:0.951458\n",
      "[98]\teval-rmspe:0.949903\ttrain-rmspe:0.949946\n",
      "[99]\teval-rmspe:0.948356\ttrain-rmspe:0.948403\n",
      "[100]\teval-rmspe:0.946781\ttrain-rmspe:0.94683\n",
      "[101]\teval-rmspe:0.945175\ttrain-rmspe:0.945226\n",
      "[102]\teval-rmspe:0.943538\ttrain-rmspe:0.943591\n",
      "[103]\teval-rmspe:0.941868\ttrain-rmspe:0.941925\n",
      "[104]\teval-rmspe:0.940168\ttrain-rmspe:0.940227\n",
      "[105]\teval-rmspe:0.938433\ttrain-rmspe:0.938495\n",
      "[106]\teval-rmspe:0.936666\ttrain-rmspe:0.936732\n",
      "[107]\teval-rmspe:0.934867\ttrain-rmspe:0.934937\n",
      "[108]\teval-rmspe:0.93304\ttrain-rmspe:0.933113\n",
      "[109]\teval-rmspe:0.931182\ttrain-rmspe:0.931258\n",
      "[110]\teval-rmspe:0.929287\ttrain-rmspe:0.929366\n",
      "[111]\teval-rmspe:0.927362\ttrain-rmspe:0.927446\n",
      "[112]\teval-rmspe:0.925406\ttrain-rmspe:0.925494\n",
      "[113]\teval-rmspe:0.923416\ttrain-rmspe:0.923508\n",
      "[114]\teval-rmspe:0.921392\ttrain-rmspe:0.92149\n",
      "[115]\teval-rmspe:0.919343\ttrain-rmspe:0.919444\n",
      "[116]\teval-rmspe:0.917258\ttrain-rmspe:0.917365\n",
      "[117]\teval-rmspe:0.915144\ttrain-rmspe:0.915257\n",
      "[118]\teval-rmspe:0.912999\ttrain-rmspe:0.913116\n",
      "[119]\teval-rmspe:0.910823\ttrain-rmspe:0.910946\n",
      "[120]\teval-rmspe:0.908613\ttrain-rmspe:0.908741\n",
      "[121]\teval-rmspe:0.906375\ttrain-rmspe:0.906509\n",
      "[122]\teval-rmspe:0.904106\ttrain-rmspe:0.904245\n",
      "[123]\teval-rmspe:0.901807\ttrain-rmspe:0.901952\n",
      "[124]\teval-rmspe:0.899477\ttrain-rmspe:0.899627\n",
      "[125]\teval-rmspe:0.897116\ttrain-rmspe:0.897273\n",
      "[126]\teval-rmspe:0.894723\ttrain-rmspe:0.894887\n",
      "[127]\teval-rmspe:0.892298\ttrain-rmspe:0.89247\n",
      "[128]\teval-rmspe:0.889844\ttrain-rmspe:0.890022\n",
      "[129]\teval-rmspe:0.887362\ttrain-rmspe:0.887548\n",
      "[130]\teval-rmspe:0.884847\ttrain-rmspe:0.885041\n",
      "[131]\teval-rmspe:0.882306\ttrain-rmspe:0.882508\n",
      "[132]\teval-rmspe:0.879733\ttrain-rmspe:0.879943\n",
      "[133]\teval-rmspe:0.877132\ttrain-rmspe:0.877352\n",
      "[134]\teval-rmspe:0.874503\ttrain-rmspe:0.874732\n",
      "[135]\teval-rmspe:0.871851\ttrain-rmspe:0.872089\n",
      "[136]\teval-rmspe:0.869164\ttrain-rmspe:0.869412\n",
      "[137]\teval-rmspe:0.866452\ttrain-rmspe:0.866708\n",
      "[138]\teval-rmspe:0.863711\ttrain-rmspe:0.863978\n",
      "[139]\teval-rmspe:0.860944\ttrain-rmspe:0.86122\n",
      "[140]\teval-rmspe:0.858148\ttrain-rmspe:0.858436\n",
      "[141]\teval-rmspe:0.855323\ttrain-rmspe:0.855624\n",
      "[142]\teval-rmspe:0.852476\ttrain-rmspe:0.852787\n",
      "[143]\teval-rmspe:0.849605\ttrain-rmspe:0.849926\n",
      "[144]\teval-rmspe:0.846708\ttrain-rmspe:0.847042\n",
      "[145]\teval-rmspe:0.843789\ttrain-rmspe:0.844135\n",
      "[146]\teval-rmspe:0.840841\ttrain-rmspe:0.841201\n",
      "[147]\teval-rmspe:0.837873\ttrain-rmspe:0.838246\n",
      "[148]\teval-rmspe:0.834878\ttrain-rmspe:0.835265\n",
      "[149]\teval-rmspe:0.831863\ttrain-rmspe:0.832264\n",
      "[150]\teval-rmspe:0.828825\ttrain-rmspe:0.82924\n",
      "[151]\teval-rmspe:0.825766\ttrain-rmspe:0.826195\n",
      "[152]\teval-rmspe:0.822681\ttrain-rmspe:0.823126\n",
      "[153]\teval-rmspe:0.819577\ttrain-rmspe:0.820038\n",
      "[154]\teval-rmspe:0.816448\ttrain-rmspe:0.816926\n",
      "[155]\teval-rmspe:0.813298\ttrain-rmspe:0.813794\n",
      "[156]\teval-rmspe:0.810124\ttrain-rmspe:0.81064\n",
      "[157]\teval-rmspe:0.80694\ttrain-rmspe:0.807473\n",
      "[158]\teval-rmspe:0.803734\ttrain-rmspe:0.804286\n",
      "[159]\teval-rmspe:0.80051\ttrain-rmspe:0.80108\n",
      "[160]\teval-rmspe:0.797263\ttrain-rmspe:0.797854\n",
      "[161]\teval-rmspe:0.794\ttrain-rmspe:0.794611\n",
      "[162]\teval-rmspe:0.790724\ttrain-rmspe:0.791355\n",
      "[163]\teval-rmspe:0.787426\ttrain-rmspe:0.78808\n",
      "[164]\teval-rmspe:0.784117\ttrain-rmspe:0.784792\n",
      "[165]\teval-rmspe:0.780785\ttrain-rmspe:0.781483\n",
      "[166]\teval-rmspe:0.777437\ttrain-rmspe:0.778159\n",
      "[167]\teval-rmspe:0.774076\ttrain-rmspe:0.774824\n",
      "[168]\teval-rmspe:0.770697\ttrain-rmspe:0.771472\n",
      "[169]\teval-rmspe:0.767307\ttrain-rmspe:0.768108\n",
      "[170]\teval-rmspe:0.763906\ttrain-rmspe:0.764734\n",
      "[171]\teval-rmspe:0.760487\ttrain-rmspe:0.761343\n",
      "[172]\teval-rmspe:0.75706\ttrain-rmspe:0.757944\n",
      "[173]\teval-rmspe:0.753624\ttrain-rmspe:0.754536\n",
      "[174]\teval-rmspe:0.750173\ttrain-rmspe:0.751114\n",
      "[175]\teval-rmspe:0.746713\ttrain-rmspe:0.747685\n",
      "[176]\teval-rmspe:0.743246\ttrain-rmspe:0.744248\n",
      "[177]\teval-rmspe:0.739765\ttrain-rmspe:0.7408\n",
      "[178]\teval-rmspe:0.736284\ttrain-rmspe:0.737348\n",
      "[179]\teval-rmspe:0.732782\ttrain-rmspe:0.73388\n",
      "[180]\teval-rmspe:0.729281\ttrain-rmspe:0.73041\n",
      "[181]\teval-rmspe:0.725769\ttrain-rmspe:0.726933\n",
      "[182]\teval-rmspe:0.722249\ttrain-rmspe:0.723448\n",
      "[183]\teval-rmspe:0.718722\ttrain-rmspe:0.719959\n",
      "[184]\teval-rmspe:0.715191\ttrain-rmspe:0.716465\n",
      "[185]\teval-rmspe:0.711658\ttrain-rmspe:0.712966\n",
      "[186]\teval-rmspe:0.708116\ttrain-rmspe:0.709463\n",
      "[187]\teval-rmspe:0.704572\ttrain-rmspe:0.705957\n",
      "[188]\teval-rmspe:0.701024\ttrain-rmspe:0.702447\n",
      "[189]\teval-rmspe:0.69747\ttrain-rmspe:0.698937\n",
      "[190]\teval-rmspe:0.69391\ttrain-rmspe:0.695418\n",
      "[191]\teval-rmspe:0.69035\ttrain-rmspe:0.691902\n",
      "[192]\teval-rmspe:0.686789\ttrain-rmspe:0.688386\n",
      "[193]\teval-rmspe:0.683229\ttrain-rmspe:0.684869\n",
      "[194]\teval-rmspe:0.679671\ttrain-rmspe:0.681347\n",
      "[195]\teval-rmspe:0.676111\ttrain-rmspe:0.677833\n",
      "[196]\teval-rmspe:0.672553\ttrain-rmspe:0.674321\n",
      "[197]\teval-rmspe:0.66899\ttrain-rmspe:0.670802\n",
      "[198]\teval-rmspe:0.665428\ttrain-rmspe:0.667293\n",
      "[199]\teval-rmspe:0.661874\ttrain-rmspe:0.663786\n",
      "[200]\teval-rmspe:0.658318\ttrain-rmspe:0.660282\n",
      "[201]\teval-rmspe:0.654766\ttrain-rmspe:0.656781\n",
      "[202]\teval-rmspe:0.651217\ttrain-rmspe:0.653286\n",
      "[203]\teval-rmspe:0.647671\ttrain-rmspe:0.649796\n",
      "[204]\teval-rmspe:0.644129\ttrain-rmspe:0.646311\n",
      "[205]\teval-rmspe:0.640592\ttrain-rmspe:0.642829\n",
      "[206]\teval-rmspe:0.637062\ttrain-rmspe:0.639356\n",
      "[207]\teval-rmspe:0.633538\ttrain-rmspe:0.635889\n",
      "[208]\teval-rmspe:0.630023\ttrain-rmspe:0.63243\n",
      "[209]\teval-rmspe:0.626513\ttrain-rmspe:0.628981\n",
      "[210]\teval-rmspe:0.623009\ttrain-rmspe:0.625538\n",
      "[211]\teval-rmspe:0.61951\ttrain-rmspe:0.622106\n",
      "[212]\teval-rmspe:0.616022\ttrain-rmspe:0.618683\n",
      "[213]\teval-rmspe:0.612536\ttrain-rmspe:0.615261\n",
      "[214]\teval-rmspe:0.609062\ttrain-rmspe:0.611856\n",
      "[215]\teval-rmspe:0.605604\ttrain-rmspe:0.608445\n",
      "[216]\teval-rmspe:0.602153\ttrain-rmspe:0.60506\n",
      "[217]\teval-rmspe:0.598712\ttrain-rmspe:0.601687\n",
      "[218]\teval-rmspe:0.595284\ttrain-rmspe:0.598329\n",
      "[219]\teval-rmspe:0.591864\ttrain-rmspe:0.594977\n",
      "[220]\teval-rmspe:0.588455\ttrain-rmspe:0.591645\n",
      "[221]\teval-rmspe:0.585057\ttrain-rmspe:0.588322\n",
      "[222]\teval-rmspe:0.581674\ttrain-rmspe:0.585013\n",
      "[223]\teval-rmspe:0.578305\ttrain-rmspe:0.58172\n",
      "[224]\teval-rmspe:0.574947\ttrain-rmspe:0.578441\n",
      "[225]\teval-rmspe:0.571607\ttrain-rmspe:0.575175\n",
      "[226]\teval-rmspe:0.568276\ttrain-rmspe:0.571928\n",
      "[227]\teval-rmspe:0.564961\ttrain-rmspe:0.568695\n",
      "[228]\teval-rmspe:0.561663\ttrain-rmspe:0.565479\n",
      "[229]\teval-rmspe:0.55838\ttrain-rmspe:0.562281\n",
      "[230]\teval-rmspe:0.555111\ttrain-rmspe:0.559095\n",
      "[231]\teval-rmspe:0.55186\ttrain-rmspe:0.555931\n",
      "[232]\teval-rmspe:0.548624\ttrain-rmspe:0.552782\n",
      "[233]\teval-rmspe:0.545404\ttrain-rmspe:0.549651\n",
      "[234]\teval-rmspe:0.542201\ttrain-rmspe:0.546537\n",
      "[235]\teval-rmspe:0.539015\ttrain-rmspe:0.54344\n",
      "[236]\teval-rmspe:0.535848\ttrain-rmspe:0.540364\n",
      "[237]\teval-rmspe:0.532695\ttrain-rmspe:0.537304\n",
      "[238]\teval-rmspe:0.529565\ttrain-rmspe:0.534263\n",
      "[239]\teval-rmspe:0.526451\ttrain-rmspe:0.531245\n",
      "[240]\teval-rmspe:0.523358\ttrain-rmspe:0.528247\n",
      "[241]\teval-rmspe:0.520281\ttrain-rmspe:0.525267\n",
      "[242]\teval-rmspe:0.517227\ttrain-rmspe:0.52231\n",
      "[243]\teval-rmspe:0.514194\ttrain-rmspe:0.519377\n",
      "[244]\teval-rmspe:0.51118\ttrain-rmspe:0.516466\n",
      "[245]\teval-rmspe:0.508186\ttrain-rmspe:0.513572\n",
      "[246]\teval-rmspe:0.5052\ttrain-rmspe:0.510685\n",
      "[247]\teval-rmspe:0.502247\ttrain-rmspe:0.507841\n",
      "[248]\teval-rmspe:0.499315\ttrain-rmspe:0.505015\n",
      "[249]\teval-rmspe:0.496404\ttrain-rmspe:0.502213\n",
      "[250]\teval-rmspe:0.493518\ttrain-rmspe:0.499432\n",
      "[251]\teval-rmspe:0.490655\ttrain-rmspe:0.496676\n",
      "[252]\teval-rmspe:0.487813\ttrain-rmspe:0.493946\n",
      "[253]\teval-rmspe:0.484994\ttrain-rmspe:0.491241\n",
      "[254]\teval-rmspe:0.482193\ttrain-rmspe:0.488555\n",
      "[255]\teval-rmspe:0.479418\ttrain-rmspe:0.485895\n",
      "[256]\teval-rmspe:0.476664\ttrain-rmspe:0.483257\n",
      "[257]\teval-rmspe:0.473934\ttrain-rmspe:0.480643\n",
      "[258]\teval-rmspe:0.471213\ttrain-rmspe:0.478047\n",
      "[259]\teval-rmspe:0.468531\ttrain-rmspe:0.475487\n",
      "[260]\teval-rmspe:0.465873\ttrain-rmspe:0.472946\n",
      "[261]\teval-rmspe:0.463228\ttrain-rmspe:0.470421\n",
      "[262]\teval-rmspe:0.460618\ttrain-rmspe:0.467936\n",
      "[263]\teval-rmspe:0.458033\ttrain-rmspe:0.465473\n",
      "[264]\teval-rmspe:0.455468\ttrain-rmspe:0.463022\n",
      "[265]\teval-rmspe:0.452929\ttrain-rmspe:0.460609\n",
      "[266]\teval-rmspe:0.450399\ttrain-rmspe:0.458194\n",
      "[267]\teval-rmspe:0.447903\ttrain-rmspe:0.455807\n",
      "[268]\teval-rmspe:0.445409\ttrain-rmspe:0.453438\n",
      "[269]\teval-rmspe:0.442973\ttrain-rmspe:0.451129\n",
      "[270]\teval-rmspe:0.440557\ttrain-rmspe:0.448846\n",
      "[271]\teval-rmspe:0.43817\ttrain-rmspe:0.446586\n",
      "[272]\teval-rmspe:0.435797\ttrain-rmspe:0.444349\n",
      "[273]\teval-rmspe:0.433462\ttrain-rmspe:0.442151\n",
      "[274]\teval-rmspe:0.431154\ttrain-rmspe:0.439982\n",
      "[275]\teval-rmspe:0.428873\ttrain-rmspe:0.437838\n",
      "[276]\teval-rmspe:0.426609\ttrain-rmspe:0.435706\n",
      "[277]\teval-rmspe:0.424378\ttrain-rmspe:0.433616\n",
      "[278]\teval-rmspe:0.422172\ttrain-rmspe:0.431549\n",
      "[279]\teval-rmspe:0.419969\ttrain-rmspe:0.429488\n",
      "[280]\teval-rmspe:0.417781\ttrain-rmspe:0.427414\n",
      "[281]\teval-rmspe:0.415641\ttrain-rmspe:0.42542\n",
      "[282]\teval-rmspe:0.413526\ttrain-rmspe:0.423451\n",
      "[283]\teval-rmspe:0.411459\ttrain-rmspe:0.421525\n",
      "[284]\teval-rmspe:0.409399\ttrain-rmspe:0.419602\n",
      "[285]\teval-rmspe:0.40737\ttrain-rmspe:0.417718\n",
      "[286]\teval-rmspe:0.405364\ttrain-rmspe:0.41585\n",
      "[287]\teval-rmspe:0.40339\ttrain-rmspe:0.414018\n",
      "[288]\teval-rmspe:0.401448\ttrain-rmspe:0.412223\n",
      "[289]\teval-rmspe:0.39952\ttrain-rmspe:0.410438\n",
      "[290]\teval-rmspe:0.397621\ttrain-rmspe:0.408686\n",
      "[291]\teval-rmspe:0.395737\ttrain-rmspe:0.406937\n",
      "[292]\teval-rmspe:0.393898\ttrain-rmspe:0.405241\n",
      "[293]\teval-rmspe:0.392072\ttrain-rmspe:0.403561\n",
      "[294]\teval-rmspe:0.390277\ttrain-rmspe:0.401916\n",
      "[295]\teval-rmspe:0.388512\ttrain-rmspe:0.400303\n",
      "[296]\teval-rmspe:0.386746\ttrain-rmspe:0.398678\n",
      "[297]\teval-rmspe:0.385012\ttrain-rmspe:0.397091\n",
      "[298]\teval-rmspe:0.383309\ttrain-rmspe:0.39554\n",
      "[299]\teval-rmspe:0.381645\ttrain-rmspe:0.394021\n",
      "[300]\teval-rmspe:0.379995\ttrain-rmspe:0.392521\n",
      "[301]\teval-rmspe:0.378365\ttrain-rmspe:0.391028\n",
      "[302]\teval-rmspe:0.376767\ttrain-rmspe:0.389576\n",
      "[303]\teval-rmspe:0.375173\ttrain-rmspe:0.388127\n",
      "[304]\teval-rmspe:0.373626\ttrain-rmspe:0.386722\n",
      "[305]\teval-rmspe:0.372124\ttrain-rmspe:0.385371\n",
      "[306]\teval-rmspe:0.370639\ttrain-rmspe:0.384034\n",
      "[307]\teval-rmspe:0.369181\ttrain-rmspe:0.382727\n",
      "[308]\teval-rmspe:0.367751\ttrain-rmspe:0.381442\n",
      "[309]\teval-rmspe:0.366344\ttrain-rmspe:0.380189\n",
      "[310]\teval-rmspe:0.364914\ttrain-rmspe:0.378901\n",
      "[311]\teval-rmspe:0.363531\ttrain-rmspe:0.377661\n",
      "[312]\teval-rmspe:0.362201\ttrain-rmspe:0.376478\n",
      "[313]\teval-rmspe:0.360869\ttrain-rmspe:0.375285\n",
      "[314]\teval-rmspe:0.359581\ttrain-rmspe:0.374144\n",
      "[315]\teval-rmspe:0.358324\ttrain-rmspe:0.37303\n",
      "[316]\teval-rmspe:0.357041\ttrain-rmspe:0.371895\n",
      "[317]\teval-rmspe:0.355773\ttrain-rmspe:0.370764\n",
      "[318]\teval-rmspe:0.354576\ttrain-rmspe:0.369716\n",
      "[319]\teval-rmspe:0.353396\ttrain-rmspe:0.368682\n",
      "[320]\teval-rmspe:0.352199\ttrain-rmspe:0.367631\n",
      "[321]\teval-rmspe:0.35106\ttrain-rmspe:0.366638\n",
      "[322]\teval-rmspe:0.349915\ttrain-rmspe:0.36564\n",
      "[323]\teval-rmspe:0.348807\ttrain-rmspe:0.364671\n",
      "[324]\teval-rmspe:0.347745\ttrain-rmspe:0.363767\n",
      "[325]\teval-rmspe:0.346708\ttrain-rmspe:0.362874\n",
      "[326]\teval-rmspe:0.345648\ttrain-rmspe:0.361939\n",
      "[327]\teval-rmspe:0.344607\ttrain-rmspe:0.361055\n",
      "[328]\teval-rmspe:0.343611\ttrain-rmspe:0.360206\n",
      "[329]\teval-rmspe:0.342659\ttrain-rmspe:0.359392\n",
      "[330]\teval-rmspe:0.341693\ttrain-rmspe:0.358555\n",
      "[331]\teval-rmspe:0.340757\ttrain-rmspe:0.357749\n",
      "[332]\teval-rmspe:0.339854\ttrain-rmspe:0.356969\n",
      "[333]\teval-rmspe:0.338954\ttrain-rmspe:0.356209\n",
      "[334]\teval-rmspe:0.338079\ttrain-rmspe:0.355469\n",
      "[335]\teval-rmspe:0.337244\ttrain-rmspe:0.354747\n",
      "[336]\teval-rmspe:0.336436\ttrain-rmspe:0.354066\n",
      "[337]\teval-rmspe:0.335624\ttrain-rmspe:0.353381\n",
      "[338]\teval-rmspe:0.33483\ttrain-rmspe:0.352712\n",
      "[339]\teval-rmspe:0.334048\ttrain-rmspe:0.352059\n",
      "[340]\teval-rmspe:0.333318\ttrain-rmspe:0.351461\n",
      "[341]\teval-rmspe:0.332599\ttrain-rmspe:0.350863\n",
      "[342]\teval-rmspe:0.331854\ttrain-rmspe:0.350256\n",
      "[343]\teval-rmspe:0.331131\ttrain-rmspe:0.349644\n",
      "[344]\teval-rmspe:0.330477\ttrain-rmspe:0.349104\n",
      "[345]\teval-rmspe:0.329839\ttrain-rmspe:0.348589\n",
      "[346]\teval-rmspe:0.329182\ttrain-rmspe:0.348058\n",
      "[347]\teval-rmspe:0.328534\ttrain-rmspe:0.347546\n",
      "[348]\teval-rmspe:0.327917\ttrain-rmspe:0.347071\n",
      "[349]\teval-rmspe:0.32731\ttrain-rmspe:0.34658\n",
      "[350]\teval-rmspe:0.326705\ttrain-rmspe:0.346095\n",
      "[351]\teval-rmspe:0.326109\ttrain-rmspe:0.345619\n",
      "[352]\teval-rmspe:0.325559\ttrain-rmspe:0.345142\n",
      "[353]\teval-rmspe:0.324976\ttrain-rmspe:0.344694\n",
      "[354]\teval-rmspe:0.324451\ttrain-rmspe:0.344289\n",
      "[355]\teval-rmspe:0.323927\ttrain-rmspe:0.343878\n",
      "[356]\teval-rmspe:0.323423\ttrain-rmspe:0.343504\n",
      "[357]\teval-rmspe:0.322935\ttrain-rmspe:0.343102\n",
      "[358]\teval-rmspe:0.322478\ttrain-rmspe:0.342758\n",
      "[359]\teval-rmspe:0.322041\ttrain-rmspe:0.34243\n",
      "[360]\teval-rmspe:0.321601\ttrain-rmspe:0.342099\n",
      "[361]\teval-rmspe:0.32117\ttrain-rmspe:0.341774\n",
      "[362]\teval-rmspe:0.320738\ttrain-rmspe:0.341441\n",
      "[363]\teval-rmspe:0.320322\ttrain-rmspe:0.341104\n",
      "[364]\teval-rmspe:0.319877\ttrain-rmspe:0.340774\n",
      "[365]\teval-rmspe:0.31951\ttrain-rmspe:0.340515\n",
      "[366]\teval-rmspe:0.319124\ttrain-rmspe:0.340245\n",
      "[367]\teval-rmspe:0.318737\ttrain-rmspe:0.339942\n",
      "[368]\teval-rmspe:0.318409\ttrain-rmspe:0.33971\n",
      "[369]\teval-rmspe:0.318112\ttrain-rmspe:0.339509\n",
      "[370]\teval-rmspe:0.317744\ttrain-rmspe:0.33925\n",
      "[371]\teval-rmspe:0.317465\ttrain-rmspe:0.339067\n",
      "[372]\teval-rmspe:0.317133\ttrain-rmspe:0.338834\n",
      "[373]\teval-rmspe:0.316842\ttrain-rmspe:0.338635\n",
      "[374]\teval-rmspe:0.316606\ttrain-rmspe:0.338495\n",
      "[375]\teval-rmspe:0.316344\ttrain-rmspe:0.338329\n",
      "[376]\teval-rmspe:0.316074\ttrain-rmspe:0.338126\n",
      "[377]\teval-rmspe:0.315805\ttrain-rmspe:0.337921\n",
      "[378]\teval-rmspe:0.31561\ttrain-rmspe:0.337814\n",
      "[379]\teval-rmspe:0.31542\ttrain-rmspe:0.337731\n",
      "[380]\teval-rmspe:0.315246\ttrain-rmspe:0.337642\n",
      "[381]\teval-rmspe:0.315004\ttrain-rmspe:0.337503\n",
      "[382]\teval-rmspe:0.314746\ttrain-rmspe:0.337354\n",
      "[383]\teval-rmspe:0.314573\ttrain-rmspe:0.337265\n",
      "[384]\teval-rmspe:0.314397\ttrain-rmspe:0.337158\n",
      "[385]\teval-rmspe:0.314223\ttrain-rmspe:0.337068\n",
      "[386]\teval-rmspe:0.314037\ttrain-rmspe:0.336969\n",
      "[387]\teval-rmspe:0.313851\ttrain-rmspe:0.336883\n",
      "[388]\teval-rmspe:0.313645\ttrain-rmspe:0.336786\n",
      "[389]\teval-rmspe:0.313463\ttrain-rmspe:0.336692\n",
      "[390]\teval-rmspe:0.313382\ttrain-rmspe:0.336678\n",
      "[391]\teval-rmspe:0.313281\ttrain-rmspe:0.336651\n",
      "[392]\teval-rmspe:0.313102\ttrain-rmspe:0.336579\n",
      "[393]\teval-rmspe:0.31302\ttrain-rmspe:0.33656\n",
      "[394]\teval-rmspe:0.312885\ttrain-rmspe:0.336515\n",
      "[395]\teval-rmspe:0.312797\ttrain-rmspe:0.336507\n",
      "[396]\teval-rmspe:0.312658\ttrain-rmspe:0.336467\n",
      "[397]\teval-rmspe:0.312501\ttrain-rmspe:0.336346\n",
      "[398]\teval-rmspe:0.312442\ttrain-rmspe:0.33637\n",
      "[399]\teval-rmspe:0.312374\ttrain-rmspe:0.336352\n",
      "[400]\teval-rmspe:0.31228\ttrain-rmspe:0.336344\n",
      "[401]\teval-rmspe:0.312162\ttrain-rmspe:0.33632\n",
      "[402]\teval-rmspe:0.312047\ttrain-rmspe:0.336289\n",
      "[403]\teval-rmspe:0.312002\ttrain-rmspe:0.336306\n",
      "[404]\teval-rmspe:0.311942\ttrain-rmspe:0.336316\n",
      "[405]\teval-rmspe:0.311893\ttrain-rmspe:0.336365\n",
      "[406]\teval-rmspe:0.311872\ttrain-rmspe:0.336421\n",
      "[407]\teval-rmspe:0.311736\ttrain-rmspe:0.336375\n",
      "[408]\teval-rmspe:0.311673\ttrain-rmspe:0.336386\n",
      "[409]\teval-rmspe:0.311608\ttrain-rmspe:0.336395\n",
      "[410]\teval-rmspe:0.311601\ttrain-rmspe:0.336445\n",
      "[411]\teval-rmspe:0.311622\ttrain-rmspe:0.336532\n",
      "[412]\teval-rmspe:0.311559\ttrain-rmspe:0.336538\n",
      "[413]\teval-rmspe:0.311561\ttrain-rmspe:0.33662\n",
      "[414]\teval-rmspe:0.311538\ttrain-rmspe:0.336685\n",
      "[415]\teval-rmspe:0.31156\ttrain-rmspe:0.336769\n",
      "[416]\teval-rmspe:0.311577\ttrain-rmspe:0.336845\n",
      "[417]\teval-rmspe:0.311526\ttrain-rmspe:0.336853\n",
      "[418]\teval-rmspe:0.311573\ttrain-rmspe:0.336954\n",
      "[419]\teval-rmspe:0.311565\ttrain-rmspe:0.337019\n",
      "[420]\teval-rmspe:0.311596\ttrain-rmspe:0.337107\n",
      "[421]\teval-rmspe:0.311663\ttrain-rmspe:0.337229\n",
      "[422]\teval-rmspe:0.311692\ttrain-rmspe:0.337304\n",
      "[423]\teval-rmspe:0.311684\ttrain-rmspe:0.337352\n",
      "[424]\teval-rmspe:0.311749\ttrain-rmspe:0.337472\n",
      "[425]\teval-rmspe:0.311807\ttrain-rmspe:0.337554\n",
      "[426]\teval-rmspe:0.31181\ttrain-rmspe:0.33761\n",
      "[427]\teval-rmspe:0.311793\ttrain-rmspe:0.337671\n",
      "[428]\teval-rmspe:0.311825\ttrain-rmspe:0.337758\n",
      "[429]\teval-rmspe:0.311918\ttrain-rmspe:0.337894\n",
      "[430]\teval-rmspe:0.311844\ttrain-rmspe:0.337893\n",
      "[431]\teval-rmspe:0.311947\ttrain-rmspe:0.338042\n",
      "[432]\teval-rmspe:0.311973\ttrain-rmspe:0.338118\n",
      "[433]\teval-rmspe:0.311973\ttrain-rmspe:0.338197\n",
      "[434]\teval-rmspe:0.312024\ttrain-rmspe:0.338305\n",
      "[435]\teval-rmspe:0.312006\ttrain-rmspe:0.338329\n",
      "[436]\teval-rmspe:0.312088\ttrain-rmspe:0.338449\n",
      "[437]\teval-rmspe:0.312189\ttrain-rmspe:0.338589\n",
      "[438]\teval-rmspe:0.312256\ttrain-rmspe:0.338695\n",
      "[439]\teval-rmspe:0.312344\ttrain-rmspe:0.338833\n",
      "[440]\teval-rmspe:0.312363\ttrain-rmspe:0.33892\n",
      "[441]\teval-rmspe:0.31223\ttrain-rmspe:0.338853\n",
      "[442]\teval-rmspe:0.312207\ttrain-rmspe:0.338888\n",
      "[443]\teval-rmspe:0.312224\ttrain-rmspe:0.338955\n",
      "[444]\teval-rmspe:0.312219\ttrain-rmspe:0.339029\n",
      "[445]\teval-rmspe:0.312245\ttrain-rmspe:0.339128\n",
      "[446]\teval-rmspe:0.312365\ttrain-rmspe:0.339294\n",
      "[447]\teval-rmspe:0.312306\ttrain-rmspe:0.339283\n",
      "[448]\teval-rmspe:0.312402\ttrain-rmspe:0.339387\n",
      "[449]\teval-rmspe:0.312489\ttrain-rmspe:0.339512\n",
      "[450]\teval-rmspe:0.31256\ttrain-rmspe:0.339621\n",
      "[451]\teval-rmspe:0.312565\ttrain-rmspe:0.339696\n",
      "[452]\teval-rmspe:0.312574\ttrain-rmspe:0.339779\n",
      "[453]\teval-rmspe:0.312714\ttrain-rmspe:0.339945\n",
      "[454]\teval-rmspe:0.312846\ttrain-rmspe:0.340108\n",
      "[455]\teval-rmspe:0.312865\ttrain-rmspe:0.340175\n",
      "[456]\teval-rmspe:0.312984\ttrain-rmspe:0.340306\n",
      "[457]\teval-rmspe:0.3131\ttrain-rmspe:0.34045\n",
      "[458]\teval-rmspe:0.313198\ttrain-rmspe:0.340583\n",
      "[459]\teval-rmspe:0.313247\ttrain-rmspe:0.340657\n",
      "[460]\teval-rmspe:0.313293\ttrain-rmspe:0.340716\n",
      "[461]\teval-rmspe:0.313261\ttrain-rmspe:0.340684\n",
      "[462]\teval-rmspe:0.313325\ttrain-rmspe:0.340786\n",
      "[463]\teval-rmspe:0.313241\ttrain-rmspe:0.340747\n",
      "[464]\teval-rmspe:0.313271\ttrain-rmspe:0.340808\n",
      "[465]\teval-rmspe:0.313307\ttrain-rmspe:0.340882\n",
      "[466]\teval-rmspe:0.313295\ttrain-rmspe:0.340915\n",
      "[467]\teval-rmspe:0.313215\ttrain-rmspe:0.34088\n",
      "[468]\teval-rmspe:0.313268\ttrain-rmspe:0.340988\n",
      "[469]\teval-rmspe:0.313342\ttrain-rmspe:0.341093\n",
      "[470]\teval-rmspe:0.313386\ttrain-rmspe:0.341169\n",
      "[471]\teval-rmspe:0.31338\ttrain-rmspe:0.341225\n",
      "[472]\teval-rmspe:0.313339\ttrain-rmspe:0.341219\n",
      "[473]\teval-rmspe:0.313456\ttrain-rmspe:0.341371\n",
      "[474]\teval-rmspe:0.313586\ttrain-rmspe:0.341523\n",
      "[475]\teval-rmspe:0.313652\ttrain-rmspe:0.341644\n",
      "[476]\teval-rmspe:0.313714\ttrain-rmspe:0.34176\n",
      "[477]\teval-rmspe:0.313735\ttrain-rmspe:0.341809\n",
      "[478]\teval-rmspe:0.31387\ttrain-rmspe:0.341962\n",
      "[479]\teval-rmspe:0.31388\ttrain-rmspe:0.342001\n",
      "[480]\teval-rmspe:0.313846\ttrain-rmspe:0.342004\n",
      "[481]\teval-rmspe:0.3138\ttrain-rmspe:0.342006\n",
      "[482]\teval-rmspe:0.313917\ttrain-rmspe:0.342137\n",
      "[483]\teval-rmspe:0.313946\ttrain-rmspe:0.342214\n",
      "[484]\teval-rmspe:0.313939\ttrain-rmspe:0.342196\n",
      "[485]\teval-rmspe:0.314034\ttrain-rmspe:0.342317\n",
      "[486]\teval-rmspe:0.314135\ttrain-rmspe:0.34246\n",
      "[487]\teval-rmspe:0.314233\ttrain-rmspe:0.342547\n",
      "[488]\teval-rmspe:0.314281\ttrain-rmspe:0.342635\n",
      "[489]\teval-rmspe:0.314384\ttrain-rmspe:0.342759\n",
      "[490]\teval-rmspe:0.314491\ttrain-rmspe:0.34289\n",
      "[491]\teval-rmspe:0.314596\ttrain-rmspe:0.343016\n",
      "[492]\teval-rmspe:0.314713\ttrain-rmspe:0.343147\n",
      "[493]\teval-rmspe:0.31463\ttrain-rmspe:0.343119\n",
      "[494]\teval-rmspe:0.314751\ttrain-rmspe:0.343245\n",
      "[495]\teval-rmspe:0.31481\ttrain-rmspe:0.343331\n",
      "[496]\teval-rmspe:0.314924\ttrain-rmspe:0.343463\n",
      "[497]\teval-rmspe:0.31495\ttrain-rmspe:0.343527\n",
      "[498]\teval-rmspe:0.315062\ttrain-rmspe:0.343662\n",
      "[499]\teval-rmspe:0.315173\ttrain-rmspe:0.343783\n",
      "[500]\teval-rmspe:0.315256\ttrain-rmspe:0.343834\n",
      "[501]\teval-rmspe:0.315291\ttrain-rmspe:0.343893\n",
      "[502]\teval-rmspe:0.315285\ttrain-rmspe:0.343915\n",
      "Stopping. Best iteration:\n",
      "[402]\teval-rmspe:0.312047\ttrain-rmspe:0.336289\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'train_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-261-4706281dd819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXGB_native\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_non_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-259-ed1b72b79b76>\u001b[0m in \u001b[0;36mXGB_native\u001b[0;34m(train, test, features, features_non_numeric)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mgbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwatchlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrmspe_xg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_probs\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtrain_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrmspe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgoal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'train_probs' is not defined"
     ]
    }
   ],
   "source": [
    "XGB_native(train,test,features,features_non_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
