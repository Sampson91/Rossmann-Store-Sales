{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "import operator\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta,date\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./train.csv\", low_memory=False)\n",
    "df_test = pd.read_csv(\"./test.csv\", low_memory=False)\n",
    "df_store = pd.read_csv(\"./store.csv\", low_memory=False)\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:138: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add additional past_quater_mean_sale and past_year_mean_sale\n",
      "Merging\n",
      "Final output\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "store = pd.read_csv('./store.csv')\n",
    "#1、去除销量中的极值,由于无法估计极值对结果的影响，所以拟合模型的时候可以进行两次，去除极值和未去除极值\n",
    "#再试一下标准差标准误差\n",
    "# def rm_outliers(df): \n",
    "#     q1 = np.percentile(df['Sales'], 25, axis=0)\n",
    "#     q3 = np.percentile(df['Sales'], 75, axis=0)\n",
    "#     k = 2.5\n",
    "#     iqr = q3 - q1\n",
    "#     df = df[df['Sales'] > q1 - k*iqr]\n",
    "#     df = df[df['Sales'] < q3 + k*iqr]\n",
    "#     return df\n",
    "\n",
    "# def rm_outliers_std(df): \n",
    "#     std = df['Sales'].std()\n",
    "#     mean = df['Sales'].mean()\n",
    "#     k = 3\n",
    "#     df = df[df['Sales'] >  mean - k*std]\n",
    "#     df = df[df['Sales'] < mean + k*std]\n",
    "#     return df\n",
    "\n",
    "#2、对时间的拆分\n",
    "def data_processing(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['DayOfYear'] = df['Date'].apply(lambda x: x.dayofyear)\n",
    "    df['WeekOfYear'] = df['Date'].apply(lambda x: x.week)\n",
    "    df['Month'] = df['Date'].apply(lambda x: x.month)\n",
    "    df['DayOfMonth'] = df['Date'].apply(lambda x: x.day)\n",
    "    df['Year'] = df['Date'].apply(lambda x: x.year)\n",
    "    return df\n",
    "\n",
    "#4、为每个日期添加过去一个季度，过去半年，过去一年，过去两年的这家店的平均日销售量\n",
    "def store_sales_each_day(sale):\n",
    "    \n",
    "    def add_mean(store,sale,current_date,past_time):\n",
    "        past_date = current_date - timedelta(days=past_time)\n",
    "        mean_sale = sale[(sale['Date'] < current_date) & (sale['Date'] > past_date) & (sale['Store'] == store)]['Sales'].mean()\n",
    "        return mean_sale\n",
    "    \n",
    "    sale['past_quater_mean_sale'] = sale.apply(lambda row: add_mean(row['Store'], sale, row['Date'], 90), axis=1)\n",
    "    sale['past_year_mean_sale'] = sale.apply(lambda row: add_mean(row['Store'], sale, row['Date'], 183), axis=1)\n",
    "    \n",
    "    return sale\n",
    "\n",
    "#测试集调整\n",
    "def store_sales_each_day_for_test(sale,test):\n",
    "    \n",
    "    def add_mean(store,sale,current_date,past_time):\n",
    "        past_date = current_date - timedelta(days=past_time)\n",
    "        mean_sale = sale[(sale['Date'] < current_date) & (sale['Date'] > past_date) & (sale['Store'] == store)]['Sales'].mean()\n",
    "        return mean_sale\n",
    "    \n",
    "    test['past_quater_mean_sale'] = test.apply(lambda row: add_mean(row['Store'], sale, row['Date'], 90), axis=1)\n",
    "    test['past_year_mean_sale'] = test.apply(lambda row: add_mean(row['Store'], sale, row['Date'], 183), axis=1)\n",
    "\n",
    "    return test\n",
    "\n",
    "#3、为每家店添加销售量客流量相关的均值,执行顺序在对时间进行拆分后\n",
    "def add_mean_for_store(sales,store_df=store) :\n",
    "    mean_sales_promo = []\n",
    "    mean_sales_no_promo = []\n",
    "    mean_sales = []\n",
    "    mean_sales_2013 = []\n",
    "    mean_sales_2014 = []\n",
    "    mean_sales_2015 = []\n",
    "    mean_store_sales_1month = []\n",
    "    mean_store_sales_2months = []\n",
    "    mean_store_sales_3months = []\n",
    "    mean_store_sales_6months = []\n",
    "    \n",
    "    \n",
    "    mean_customers_promo = []\n",
    "    mean_customers_no_promo = []\n",
    "    mean_customers = []\n",
    "    mean_customers_2013 = []\n",
    "    mean_customers_2014 = []\n",
    "    mean_customers_2015 = []\n",
    "    mean_customers_1month = [] \n",
    "    mean_customers_2months = [] \n",
    "    mean_customers_3months = []\n",
    "    mean_customers_6months = []\n",
    "    \n",
    "    \n",
    "    for store in store_df['Store']:\n",
    "        sale = sales[sales['Store']==store]\n",
    "        \n",
    "        # mean of sales\n",
    "        mean_sales.append(sale['Sales'].mean())\n",
    "        mean_sales_promo.append(sale[sale['Promo'] == 1]['Sales'].mean())\n",
    "        mean_sales_no_promo.append(sale[sale['Promo'] == 0]['Sales'].mean())\n",
    "        mean_sales_2013.append(sale[sale['Year'] == 2013]['Sales'].mean())\n",
    "        mean_sales_2014.append(sale[sale['Year'] == 2014]['Sales'].mean())\n",
    "        mean_sales_2015.append(sale[sale['Year'] == 2015]['Sales'].mean())\n",
    "        mean_store_sales_1month.append(sale[(sale['Month'] == 7) & (sale['Year'] == 2015)]['Sales'].mean())                   \n",
    "        mean_store_sales_2months.append(sale[(sale['Month'] <= 7) ^(sale['Month'] >= 6) & (sale['Year'] == 2015)]['Sales'].mean())\n",
    "        mean_store_sales_3months.append(sale[(sale['Month'] <= 7) ^(sale['Month'] >= 5) & (sale['Year'] == 2015)]['Sales'].mean())        \n",
    "        mean_store_sales_6months.append(sale[(sale['Month'] <= 7) ^(sale['Month'] >= 2) & (sale['Year'] == 2015)]['Sales'].mean())\n",
    "        \n",
    "        # mean of customers\n",
    "        mean_customers.append(sale['Customers'].mean())\n",
    "        mean_customers_promo.append(sale[sale['Promo'] == 1]['Customers'].mean())\n",
    "        mean_customers_no_promo.append(sale[sale['Promo'] == 0]['Customers'].mean())\n",
    "        mean_customers_2013.append(sale[sale['Year'] == 2013]['Customers'].mean())\n",
    "        mean_customers_2014.append(sale[sale['Year'] == 2014]['Customers'].mean())\n",
    "        mean_customers_2015.append(sale[sale['Year'] == 2015]['Customers'].mean())\n",
    "        mean_customers_1month.append(sale[(sale['Month'] == 7) & (sale['Year'] == 2015)]['Customers'].mean())                   \n",
    "        mean_customers_2months.append(sale[(sale['Month'] <= 7) ^(sale['Month'] >= 6) & (sale['Year'] == 2015)]['Customers'].mean())\n",
    "        mean_customers_3months.append(sale[(sale['Month'] <= 7) ^(sale['Month'] >= 5) & (sale['Year'] == 2015)]['Customers'].mean())        \n",
    "        mean_customers_6months.append(sale[(sale['Month'] <= 7) ^(sale['Month'] >= 2) & (sale['Year'] == 2015)]['Customers'].mean())\n",
    "        \n",
    "    store_df['mean_sales'] = mean_sales\n",
    "    store_df['mean_sales_promo'] = mean_sales_promo\n",
    "    store_df['mean_sales_no_promo'] = mean_sales_no_promo\n",
    "    store_df['mean_sales_2013'] = mean_sales_2013\n",
    "    store_df['mean_sales_2014'] = mean_sales_2014\n",
    "    store_df['mean_sales_2015'] = mean_sales_2015\n",
    "    store_df['mean_store_sales_1month'] = mean_store_sales_1month\n",
    "    store_df['mean_store_sales_2months'] = mean_store_sales_2months\n",
    "    store_df['mean_store_sales_3months'] = mean_store_sales_3months\n",
    "    store_df['mean_store_sales_6months'] = mean_store_sales_6months\n",
    "    \n",
    "    store_df['mean_customers'] = mean_customers\n",
    "    store_df['mean_customers_promo'] = mean_customers_promo\n",
    "    store_df['mean_customers_no_promo'] = mean_customers_no_promo\n",
    "    store_df['mean_customers_2013'] = mean_customers_2013\n",
    "    store_df['mean_customers_2014'] = mean_customers_2014\n",
    "    store_df['mean_customers_2015'] = mean_customers_2015\n",
    "    store_df['mean_customers_1month'] = mean_customers_1month\n",
    "    store_df['mean_customers_2months'] = mean_customers_2months\n",
    "    store_df['mean_customers_3months'] = mean_customers_3months\n",
    "    store_df['mean_customers_6months'] = mean_customers_6months\n",
    "    \n",
    "    return store_df\n",
    "\n",
    "def drop_stores(data_test, data):\n",
    "\tstores = data_test['Store'].unique()         \n",
    "\treturn data[data_test['Store'].isin(stores)]\n",
    "\n",
    "#合并销售和商店 \n",
    "def merge_sale(sale_data, store_data):\n",
    "    train = sale_data.join(store_data, on='Store', rsuffix='_')\n",
    "    train = train.drop('Store_',axis=1)\n",
    "    return train\n",
    "\n",
    "#添加其他特征\n",
    "def extra_features(data):\n",
    "    data['CompetitionOpen'] = 12 * (data['Year'] - data['CompetitionOpenSinceYear']) + (data['Month'] - data['CompetitionOpenSinceMonth'])\n",
    "    data['PromoOpen'] = 12 * (data['Year'] - data['Promo2SinceYear'])+ (data['WeekOfYear'] - data['Promo2SinceWeek']) / 4.0\n",
    "    data['PromoOpen'] = data['PromoOpen'].apply(lambda x: x if x > 0 else 0)\n",
    "    data.loc[data.Promo2SinceYear == 0, 'PromoOpen'] = 0\n",
    "    data = data.drop(['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear'], axis=1) #删除特征\n",
    "    \n",
    "    mappings = {'0': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4,'Jan,Apr,Jul,Oct':1,'Feb,May,Aug,Nov':2,'Mar,Jun,Sept,Dec':3}  \n",
    "    data['StoreType'].replace(mappings, inplace=True)  \n",
    "    data['Assortment'].replace(mappings, inplace=True)  \n",
    "    data['StateHoliday'].replace(mappings, inplace=True)\n",
    "    data['PromoInterval'].replace(mappings, inplace=True)\n",
    "#     data = data.drop(['Date'],axis= 1)\n",
    "    return data\n",
    "\n",
    "#去除极大极小值\n",
    "# print('Moving outliers')\n",
    "# train = rm_outliers(train)\n",
    "\n",
    "#转换年月日\n",
    "print('Convert Time')\n",
    "train = data_processing(train)\n",
    "test = data_processing(test)\n",
    "\n",
    "#给商店计算平均销售量\n",
    "store = add_mean_for_store(train)\n",
    "store = drop_stores(test, store)\n",
    "\n",
    "print('add additional past_quater_mean_sale and past_year_mean_sale')\n",
    "#添加额外特征'past_quater_mean_sale'和'past_year_mean_sale'，这段代码运行时间可能会过长\n",
    "# train = store_sales_each_day(train)\n",
    "# test = store_sales_each_day_for_test(train,test)\n",
    "\n",
    "print('Merging')\n",
    "#合并\n",
    "train = merge_sale(train, store)\n",
    "test = merge_sale(test, store)\n",
    "# train.to_csv('train_withextra.csv',index=False)\n",
    "# test.to_csv('test_withextra.csv',index=False)\n",
    "\n",
    "#额外的特征\n",
    "train = extra_features(train)\n",
    "test = extra_features(test)\n",
    "\n",
    "holidayofyear = sorted(train[train['StateHoliday'].isin([1,2,3,4])]['DayOfYear'].unique())\n",
    "def day2holiday(df,holidayofyear):\n",
    "    for holiday in holidayofyear:\n",
    "        df['DaysToHoliday' + str(holiday)] = holiday - df['DayOfYear']\n",
    "    return df\n",
    "\n",
    "#计算距离节日的日子数\n",
    "train = day2holiday(train,holidayofyear)\n",
    "test =  day2holiday(test,holidayofyear)\n",
    "#参考Joe Kington进行异常值检验\n",
    "# https://stackoverflow.com/questions/22354094/pythonic-way-of-detecting-outliers-in-one-dimensional-observation-data/22357811#22357811\n",
    "def mad_based_outlier(points, thresh=3.5):\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = np.median(points, axis=0)\n",
    "    diff = np.sum((points - median)**2, axis=-1)\n",
    "    diff = np.sqrt(diff)\n",
    "    med_abs_deviation = np.median(diff)\n",
    "\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "    return modified_z_score > thresh\n",
    "\n",
    "# df = train\n",
    "for i in train['Store'].unique():\n",
    "    train.loc[(train['Store'] == i) & (train['Open'] == 1), 'Outline'] =mad_based_outlier(train.loc[(train['Store'] == i) & (train['Open'] == 1)]['Sales'],2.8)\n",
    "print('Final output')\n",
    "#生成最终输入\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = train.columns\n",
    "a2 = ['Date','Sales','Customers','Outline']\n",
    "a1 = [f for f in a1 if f not in a2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考\n",
    "# https://www.kaggle.com/mmueller/liberty-mutual-group-property-inspection-prediction/xgb-feature-importance-python/cod\n",
    "\n",
    "def ToWeight(y):\n",
    "    w = np.zeros(y.shape, dtype=float)\n",
    "    ind = y != 0\n",
    "    w[ind] = 1./(y[ind]**2)\n",
    "    return w\n",
    "\n",
    "def rmspe(yhat, y):\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "    return rmspe\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    # y = y.values\n",
    "    y = y.get_label()\n",
    "    y = np.exp(y) - 1\n",
    "    yhat = np.exp(yhat) - 1\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))\n",
    "    return \"rmspe\", rmspe\n",
    "#时间划分\n",
    "def split_train(train,days = 42):  #分割数据集\n",
    "    end_date = date(2015,7,31)\n",
    "    begin_date = date(2013,1,1)\n",
    "    split_data = str(end_date-timedelta(days))\n",
    "    test  = train[train['Date']>split_data]\n",
    "    train1 = train[train['Date']<split_data]\n",
    "    return train1,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017209 826902\n"
     ]
    }
   ],
   "source": [
    "df = train\n",
    "df = df.loc[(df['Outline'] == False) & (df['Open'] == 1)]\n",
    "\n",
    "print(len(train),len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 9, 'eta': 0.1, 'subsample': 1, 'alpha': 2, 'gamma': 2, 'colsample_bytree': 0.7, 'silent': 1, 'objective': 'reg:linear', 'seed': 1301, 'lambda': 2, 'tree_method': 'gpu_hist'}\n",
      "[0]\ttrain-rmse:7.43177\teval-rmse:7.43292\ttrain-rmspe:0.999517\teval-rmspe:0.999517\n",
      "Multiple eval metrics have been passed: 'eval-rmspe' will be used for early stopping.\n",
      "\n",
      "Will train until eval-rmspe hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:0.159259\teval-rmse:0.162509\ttrain-rmspe:0.172102\teval-rmspe:0.175084\n",
      "[200]\ttrain-rmse:0.11789\teval-rmse:0.123282\ttrain-rmspe:0.124795\teval-rmspe:0.129155\n",
      "[300]\ttrain-rmse:0.106569\teval-rmse:0.113605\ttrain-rmspe:0.112777\teval-rmspe:0.118358\n",
      "[400]\ttrain-rmse:0.099034\teval-rmse:0.107248\ttrain-rmspe:0.104898\teval-rmspe:0.111374\n",
      "[500]\ttrain-rmse:0.095062\teval-rmse:0.104275\ttrain-rmspe:0.10091\teval-rmspe:0.108129\n",
      "[600]\ttrain-rmse:0.091986\teval-rmse:0.102132\ttrain-rmspe:0.097744\teval-rmspe:0.105748\n",
      "[700]\ttrain-rmse:0.088966\teval-rmse:0.100017\ttrain-rmspe:0.094686\teval-rmspe:0.103432\n",
      "[800]\ttrain-rmse:0.087035\teval-rmse:0.098809\ttrain-rmspe:0.092803\teval-rmspe:0.102137\n",
      "[900]\ttrain-rmse:0.085357\teval-rmse:0.097833\ttrain-rmspe:0.091007\teval-rmspe:0.101076\n",
      "[1000]\ttrain-rmse:0.08381\teval-rmse:0.097011\ttrain-rmspe:0.089355\teval-rmspe:0.100156\n",
      "[1100]\ttrain-rmse:0.082676\teval-rmse:0.096416\ttrain-rmspe:0.088073\teval-rmspe:0.099509\n",
      "[1200]\ttrain-rmse:0.081688\teval-rmse:0.095952\ttrain-rmspe:0.08696\teval-rmspe:0.09896\n",
      "[1300]\ttrain-rmse:0.080705\teval-rmse:0.095477\ttrain-rmspe:0.085837\teval-rmspe:0.098451\n",
      "[1400]\ttrain-rmse:0.079905\teval-rmse:0.095144\ttrain-rmspe:0.08491\teval-rmspe:0.098098\n",
      "[1500]\ttrain-rmse:0.079002\teval-rmse:0.094687\ttrain-rmspe:0.083882\teval-rmspe:0.09759\n",
      "[1600]\ttrain-rmse:0.078224\teval-rmse:0.09441\ttrain-rmspe:0.082882\teval-rmspe:0.097271\n",
      "[1700]\ttrain-rmse:0.077561\teval-rmse:0.094161\ttrain-rmspe:0.082133\teval-rmspe:0.096978\n",
      "[1800]\ttrain-rmse:0.076878\teval-rmse:0.093944\ttrain-rmspe:0.081272\teval-rmspe:0.096644\n",
      "[1900]\ttrain-rmse:0.076251\teval-rmse:0.093701\ttrain-rmspe:0.080509\teval-rmspe:0.096344\n",
      "[2000]\ttrain-rmse:0.075603\teval-rmse:0.093498\ttrain-rmspe:0.079747\teval-rmspe:0.096149\n",
      "[2100]\ttrain-rmse:0.075039\teval-rmse:0.093315\ttrain-rmspe:0.079085\teval-rmspe:0.095946\n",
      "[2200]\ttrain-rmse:0.074462\teval-rmse:0.093116\ttrain-rmspe:0.078409\teval-rmspe:0.09574\n",
      "[2300]\ttrain-rmse:0.0739\teval-rmse:0.09292\ttrain-rmspe:0.077747\teval-rmspe:0.095538\n",
      "[2400]\ttrain-rmse:0.073401\teval-rmse:0.092782\ttrain-rmspe:0.077175\teval-rmspe:0.095369\n",
      "[2500]\ttrain-rmse:0.072892\teval-rmse:0.092653\ttrain-rmspe:0.076555\teval-rmspe:0.095204\n",
      "[2600]\ttrain-rmse:0.072455\teval-rmse:0.092571\ttrain-rmspe:0.076036\teval-rmspe:0.095145\n",
      "[2700]\ttrain-rmse:0.072002\teval-rmse:0.092469\ttrain-rmspe:0.075518\teval-rmspe:0.095019\n",
      "[2800]\ttrain-rmse:0.071582\teval-rmse:0.092383\ttrain-rmspe:0.075015\teval-rmspe:0.094927\n",
      "[2900]\ttrain-rmse:0.071145\teval-rmse:0.092301\ttrain-rmspe:0.074474\teval-rmspe:0.094822\n",
      "[3000]\ttrain-rmse:0.070752\teval-rmse:0.092225\ttrain-rmspe:0.074009\teval-rmspe:0.094754\n",
      "[3100]\ttrain-rmse:0.070392\teval-rmse:0.092165\ttrain-rmspe:0.073588\teval-rmspe:0.094685\n",
      "[3200]\ttrain-rmse:0.07001\teval-rmse:0.092108\ttrain-rmspe:0.073103\teval-rmspe:0.094592\n",
      "[3300]\ttrain-rmse:0.069662\teval-rmse:0.09205\ttrain-rmspe:0.072677\teval-rmspe:0.094527\n",
      "[3400]\ttrain-rmse:0.069319\teval-rmse:0.091997\ttrain-rmspe:0.072265\teval-rmspe:0.094441\n",
      "[3500]\ttrain-rmse:0.06895\teval-rmse:0.091941\ttrain-rmspe:0.071819\teval-rmspe:0.094334\n",
      "[3600]\ttrain-rmse:0.068625\teval-rmse:0.091905\ttrain-rmspe:0.071423\teval-rmspe:0.094287\n",
      "[3700]\ttrain-rmse:0.068293\teval-rmse:0.09185\ttrain-rmspe:0.07103\teval-rmspe:0.094229\n",
      "[3800]\ttrain-rmse:0.067995\teval-rmse:0.091821\ttrain-rmspe:0.070697\teval-rmspe:0.094201\n",
      "[3900]\ttrain-rmse:0.067686\teval-rmse:0.091791\ttrain-rmspe:0.070344\teval-rmspe:0.094146\n",
      "[4000]\ttrain-rmse:0.067398\teval-rmse:0.091754\ttrain-rmspe:0.070012\teval-rmspe:0.094093\n",
      "[4100]\ttrain-rmse:0.067113\teval-rmse:0.091725\ttrain-rmspe:0.069681\teval-rmspe:0.094051\n",
      "[4200]\ttrain-rmse:0.066824\teval-rmse:0.091708\ttrain-rmspe:0.069331\teval-rmspe:0.094022\n",
      "[4300]\ttrain-rmse:0.066547\teval-rmse:0.091688\ttrain-rmspe:0.069021\teval-rmspe:0.093996\n",
      "[4400]\ttrain-rmse:0.066286\teval-rmse:0.091663\ttrain-rmspe:0.068717\teval-rmspe:0.09395\n",
      "[4500]\ttrain-rmse:0.066044\teval-rmse:0.091643\ttrain-rmspe:0.068443\teval-rmspe:0.093935\n",
      "[4600]\ttrain-rmse:0.065796\teval-rmse:0.091624\ttrain-rmspe:0.068166\teval-rmspe:0.093905\n",
      "[4700]\ttrain-rmse:0.065551\teval-rmse:0.091612\ttrain-rmspe:0.067888\teval-rmspe:0.093864\n",
      "[4800]\ttrain-rmse:0.065311\teval-rmse:0.091606\ttrain-rmspe:0.067605\teval-rmspe:0.093839\n",
      "[4900]\ttrain-rmse:0.065089\teval-rmse:0.091594\ttrain-rmspe:0.067344\teval-rmspe:0.093813\n",
      "[5000]\ttrain-rmse:0.064849\teval-rmse:0.09159\ttrain-rmspe:0.067075\teval-rmspe:0.093806\n",
      "[5100]\ttrain-rmse:0.064609\teval-rmse:0.091579\ttrain-rmspe:0.066792\teval-rmspe:0.093806\n",
      "Stopping. Best iteration:\n",
      "[5031]\ttrain-rmse:0.064777\teval-rmse:0.091583\ttrain-rmspe:0.066993\teval-rmspe:0.093795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 1301\n",
    "depth = 9\n",
    "eta = 0.1\n",
    "ntrees = 10000\n",
    "mcw = 5\n",
    "tsize = 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[a1],df['Sales'],test_size=tsize, random_state=seed)\n",
    "\n",
    "y_train = np.log1p(y_train)\n",
    "y_test = np.log1p(y_test)\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_test, y_test)\n",
    "    \n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "param = {'max_depth': 9,\n",
    "     'eta': 0.1,\n",
    "     'subsample': 1,\n",
    "     'alpha': 2, \n",
    "     'gamma': 2,\n",
    "     'colsample_bytree': 0.7,\n",
    "     'silent': 1,\n",
    "     'objective': 'reg:linear',\n",
    "     'seed': 1301,\n",
    "     'lambda': 2,\n",
    "     'tree_method': 'gpu_hist'}\n",
    "\n",
    "print(param)\n",
    "bst = xgb.train(param, dtrain, ntrees, watchlist, feval=rmspe_xg, verbose_eval=100, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions data set\n"
     ]
    }
   ],
   "source": [
    "# {'max_depth': 9, 'eta': 0.1, 'subsample': 1, 'alpha': 2, 'gamma': 2, 'colsample_bytree': 0.8, 'silent': 1, 'objective': 'reg:linear', 'seed': 1301, 'lambda': 2, 'tree_method': 'gpu_hist'}\n",
    "# {'max_depth': 9, 'eta': 0.1, 'subsample': 1, 'alpha': 2, 'gamma': 2, 'colsample_bytree': 0.7, 'silent': 1, 'objective': 'reg:linear', 'seed': 1301, 'lambda': 2, 'tree_method': 'gpu_hist'}\n",
    "# [4765]\ttrain-rmse:0.065362\teval-rmse:0.09156\ttrain-rmspe:0.067719\teval-rmspe:0.093572\n",
    "print(\"predictions data set\")\n",
    "myid = 'Id'\n",
    "goal = 'Sales'\n",
    "teset = test[test['Open']==1]\n",
    "test_probs = bst.predict(xgb.DMatrix(test[a1]))\n",
    "indices = test_probs < 0\n",
    "test_probs[indices] = 0\n",
    "submission = pd.DataFrame({myid: test[myid], goal: np.exp(test_probs) - 1})\n",
    "li = []\n",
    "sub = submission\n",
    "for i in np.arange(1,len(df_test)+1):\n",
    "    if len(sub[sub['Id']==i]['Sales'])>0:\n",
    "        li.append(int(sub[sub['Id']==i]['Sales']))\n",
    "    else:\n",
    "        li.append(0)\n",
    "sub = pd.DataFrame({myid: np.arange(1,len(df_test)+1), goal: li})\n",
    "sub.to_csv(\"./result/dat-xgb_d0010depth%s_eta%s_ntree%s_mcw%s_tsize%s.csv\" % (str(depth),str(eta),str(ntrees),str(mcw),str(tsize)) , index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
